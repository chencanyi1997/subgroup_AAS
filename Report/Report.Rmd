---
title: "Report"
output:
  bookdown::pdf_document2:
    citation_package: natbib
  html_document:
    df_print: paged
header-includes: 
- \usepackage{url,amssymb,amsthm,amsmath,mathrsfs,bm,color}
- \usepackage{natbib}
- \setcitestyle{round}
bibliography: refs.bib
---
\newcommand{\x}{\mathbf{x}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\calS}{\mathcal{\bm{S}}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\mR}{\mathbb{R}}
\newcommand{\tY}{\widetilde{\Y}}
# Abstract

Correct indentification of subgroups of a heterogeneous population is an important step in developing precision medicine. In this report, we develop a two-stage data-driven efficient method to detect the subgroups existing in the lung cancer population under the $p>>n$ situation. We assume the heterogeneity mainly comes from the unobserved latent factors. After adjusting for the effects of a subset of the covariates $\x_i = (X_{i1},\ldots,X_{ip})$ using the Lasso procudure, we apply the k-means algorithm to divide the heterogenous population into subgroups using dynamic programming based the elbow rule with the modified BIC criterion. Finally we verify the proposed method by applying our method to the publicly available lung cancer genomic data.

# Introduction

Biological heterogeneity is common in many diseases; heterogeneity complicates clinical management, as it is often the main reason for prognostic and therapeutic failures. Thus there have been many attempts to identify subgroups from a heterogenous population. One of the most popular methods for finding subgroups from a heterogenous population is the mixture model (see, e.g. XXX). The mixture model-based approach often needs to specify the underlying distribution for data and the number of subgroups, which is impractical. Moreover, this method is not suitable for the high dimensional setting where the number of covariates $p$ is much larger than the sample size $n$. 


# Model
In this project, we consider the following model:

$$
Y_{i} = \mu_{i} + \x_{i}^T\bbeta + \epsilon_{i}, i = 1,\cdots,n,
$$
where $Y_i$ is the response for the $i$-th subject, $\mu_i$'s are unknown subject-specific intercepts, $\bbeta = (\beta_1,\ldots,\beta_p)^T$ is the coefficients vector for the covariates $\x_i$ and $\epsilon_i$ is the error term independent of $\x_i$ with $\E(\epsilon_i) = 0$ and $\Var(\epsilon_i) = \sigma^2$. In a biomedical study, e.g., the lung cancer genomic data (XXX), $Y_i$ could be a certain phenotype associated or suvival time with some disease such as "Disease-Free Survival Time", $\x_i$ could be a set of clinical variables such as gender, age, race, and the expression levels of Agilent miRNA probes. When considering the expression levels of miRNA, the sample size $n$ is usually smaller than the dimension of covariates $p$. Thus the sparsity principle applied naturally to address the high dimension issue $p>n$ where only a subset $\calS$ of $\bbeta$ is nonzero. We assume $|\calS|=s$. 

In the situation $p<n$, \cite{Ma2015ACP} considered optimizing the following objective fucntion
$$
Q_n(\bmu,\bbeta;\lambda) = 1/2\sum_{i = 1}^n(Y_i - \mu_i-\x_i^T\bbeta)^2 + \sum_{1\leq i<j\leq n} p(|\mu_i-\mu_j|,\lambda),
$$
where $p(\cdot,\lambda)$ is a concave penalty function with a tuning parameter $\lambda>0$, and applied the alternating direction method of multipliers (ADMM) algorithm to minimize $Q_n(\bmu,\bbeta;\lambda)$. ADMM algorithm is essentially a first order algorithm, hence its convergence rate is very slow. And for $n$ is large, this issue becomes more worse because we should optimize at lease ${n \choose 2}$ parameters. A nature extension to the spirit of Shujie Ma, Huang Jian (2016) is to add $\bbeta$ to the penalty term by optimizing the following objective function
$$
\widetilde{Q}_n(\bmu,\bbeta;\lambda) = 1/2\sum_{i = 1}^n(Y_i - \mu_i-\x_i^T\bbeta)^2 + \alpha_1\sum_{1\leq i<j\leq n} p(|\mu_i-\mu_j|,\lambda) + \alpha_2 p(|\bbeta|,\omega),
$$
where $\alpha_1$ and $\alpha_2$ is the weight, and apply the ADMM algorithm, which makes the issue more severe. At this time, the ADMM algorithm has to iterate on at lease ${n \choose 2} + p$ parameters sequentially where the slow convergence rate issue stands out. 

# Algorithm

Instead we first use Lasso procedure to select related covariates from $\x_i$, then apply k-means algorithm to the adjusted $\widetilde{Y_i}$. For the Lasso procedure, we minizize the following objective function
\begin{equation}\label{eq:lasso}
(\widehat{\mu}(\lambda),\widehat{\bbeta}(\lambda)) = \arg\min_{(\mu,\bbeta)\in\mR^{p+1}} 1/2\sum_{i = 1}^n(Y_i - \mu-\x_i^T\bbeta)^2 + p(\bbeta,\lambda). 
\end{equation}
Based on this, we can adjust the common effect existing in $\x_i$. Specifically, let $\widehat{\lambda}$ be the value of the tuning parameter selected based on a data-driven procedure such as Bayesian information criterion (BIC). For brevity, write $\widehat{\bbeta} = \widehat{\bbeta}(\widehat{\lambda})$. Then let $\widetilde{Y_i} = Y_i - \x_i^T\widehat{\bbeta}$. We apply the k-means algorithm to the adjusted $\widetilde{Y_i}$.

## The choice of concave penalty function
For the choice of penalty function, we can simply use the   $\ell_1$-norm penalty  \citep[LASSO]{tibshirani1996regression}, smoothly clipped absolute deviations penalty   \citep[SCAD]{fan2001variable}  or minimax concave penalty  \citep[MCP]{zhang2010nearly}.   Interested readers may refer to \cite{hastie2015statistical} and references therein for comprehensive reviews on recent developments. In the present context we adopt  $\ell_1$-norm penalty   $p(\bbeta,\lambda) = \lambda|\bbeta|_1$ for simplicity.

## Optimization methods
The first order methods, such as  \cite{combettes2011proximal}, \cite{bach2012optimization} and \cite{tropp2010computational}, and Newton type algorithms, such as \cite{fountoulakis2014matrix} and \cite{dassios2015preconditioner}, can be used to optimize \@ref(eq:lasso).  We suggest to use  the primal dual active set method \citep{fan2014primal}, which is in spirit a generalized version of Newton type method. It usually converges  after one-step iteration if there is a very good initial value. We globalize it with continuation on regularization parameter and take a maximum vote regularization parameter selection rule   incorporated along with the continuation procedure without extra computation overhead \citep[see][]{huang2018robust}.

It remains to analysis the subgroups in $\tY = (\widetilde{Y_1},\ldots,\widetilde{Y_n})^T$. This is an unsupervised cluster problem. Many loss function can be adopted to find the cluster modes, e.g., k-means, k-median, k-modes, even the deep learing algorithm. The k-means problem is to partition data into k groups such that the sum of squared Euclidean distances to each group mean is minimized. However, the problem is NP-hard in a general Euclidean space, even when the number of clusters k is 2 \citep{Aloise2009NPhardnessOE}. The standard iterative k-means algorithm \citep{Llyod1982LeastSQ} is a widely used heuristic solution. The algorithm iteratively calculates the within-cluster sum of squared distances, modifies group membership of each point to reduce the within-cluster sum of squared distances, and computes new cluster centers until local convergence is achieved. The time complexity of this standard k-means algorithm is $O(qknp)$, where $q$ is the number of iterations, k is the number of clusters. In our context, we take k-means for its simplicity. There exists dynamic programming algorithm for optimal one-dimensional clustering \citep[see][]{Wang2011Ckmeans1ddpOK}. They implement this algorithm as an R package called Ckmeans.1d.dp. This exact dynamic programming solution with a runtime of $O(n^2k)$ to the 1-D k-means problem.

## Selection of the tuning parameters $k$ and $\lambda$

We select the tuning parameter $\lambda$ by validation in terms of mse $1/n\sum_{i= 1} (Y_i - \widehat{\mu}-\x_i^T\widehat{\bbeta})^2$. And the the number of cluster $k$ by decided by bayesian information criterion (BIC)
$$
\mathrm{BIC}(k) = \log \left[\sum_{i = 1}^n\{Y_i - \widehat{\mu_i}(k) - \x_i^T\widehat{\bbeta}\}^2/n\right] + C_n \log n/n(k + s),
$$
where $C_n = c\log\{\log(n+s)\}$ with some positive constant $c$. We take $c = 10$ by defaults.


# Emperical example

## Chemores Data Example

Lung cancer is one of the most prevalent and deadiest cancers, which can be classified into two major subtypes, small cell lung cancer (SCLC) and non-small cell lung cancer (NSCLC). NSCLC accouting 80% of all primary lung cancers, is a known heterogeneous group and its prognosis is generally poor \citep{Tsuboi2007ThePS}. In the current clinical practice, it is difficult to perform histopathological classification with small biopsies \citep{Orenstein2012RevolutionIL}. There is an urgent to analysis the subgroups existing in NSCLC population.

Publicly available lung cancer genomic data from the Chemores Cohort Study. This data is part of an integrated study of mRNA, miRNA and clinical variables to characterize the molecular distinctions between squamous cell carcinoma (SCC) and adenocarcinoma (AC) in Non Small Cell Lung Cancer (NSCLC) aside large cell lung carcinoma (LCC). Tissue samples were analysed from a cohort of 123 patients, who underwent complete surgical resection at the Institut Mutualiste Montsouris (Paris, France) between 30 January 2002 and 26 June 2006. All the patients belong to NSCLC.  The studied outcome was the "Disease-Free Survival Time". Patients were followed until the first relapse occurred or administrative censoring. In this genomic dataset, the expression levels of Agilent miRNA probes (p=939) were included from the n=123 cohort samples. The miRNA data contains normalized expression levels. See below the paper by Lazar et al. (2013) and Array Express data repository for complete description of the samples, tissue preparation, Agilent array technology, and data normalization. In addition to the genomic data, five clinical variables, also evaluated on the cohort samples, are included as continuous variable ('Age') and nominal variables ('Type','KRAS.status','EGFR.status','P53.status'). Data is available [here](http://fafner.meb.ki.se/personal/yudpaw/?page_id=11).

## Data description

```


```{r setup}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(warn = -1) # suppress warings globally

library(PRIMsrc)
library(tidyverse)
library(glmnet)
library(Amelia) # for missmap
library(Ckmeans.1d.dp)
```

There are $n = 123$ individuals with covariates $p = 946$. This meets our expectation $p>n$.

```{r}
head(Real.2[,1:10])
tail(Real.2[,1:10])
dim(Real.2)
str(Real.2[,1:10])
summary(Real.2[,1:10])
(colSums(Real.2==0)/ncol(Real.2))[1:10] ## zero-inflate effect
```

Then we explore the missing patern of this dataset.

```{r}
missmap(Real.2)
```

There are no missing data.


## Results

```{r}
x <- Real.2[,-c(1)] %>% apply(2, as.numeric)
y <- Real.2$y
cvfit <- cv.glmnet(x,y)
plot(cvfit)
beta <- coef(cvfit,s="lambda.min")
beta <- beta[2:length(beta)]
yhat <- x%*%beta
res = y - x %*% beta
res = res %>% as.data.frame
colnames(res) = "ires"
ggplot(res, aes(x = ires, y = ..density..)) + 
  geom_histogram(fill = "cornsilk", colour = "grey60", size = 0.2)+ geom_density() + xlim(-2, 1) +
  theme_classic()
```
We can find two modes in the density plot. Thus, we apply kmeans to the residual.

```{r}
computeBIC <- function(X, y, muh, betah, k, c = 10) {
  
  s <- sum(abs(betah)>0)
  n <- dim(X)[1]
  p <- dim(X)[2]
  Qn <- sum((y-muh-X%*%betah)^2)/n
  df <- k+p;
  bic <- log(Qn) + c*log(log(n+p))*log(n)/n*df;
  return(bic)
}
n <- dim(x)[1]
p <- dim(x)[2]
kL <- 1
kU <- n-1
karr <- seq(kL,kU)
bicarr <- -1*rep(1,length(karr))
for ( i in 1:length(karr)) {
 k <- karr[i]
r <- Ckmeans.1d.dp(res[[1]],k)
muh <- r$centers[r$cluster]
bicarr[i] <- computeBIC(x,y,muh,beta,i)
}
df <- data.frame(k = karr[1:10],
                 bic = bicarr[1:10])
df %>% ggplot(aes(x = k, y = bic))+
  geom_point() +
  scale_x_continuous(breaks= karr[1:10]) +
  theme_classic()
```


```{r}
cl <- kmeans(res,2)
res$cluster <- cl$cluster
res1 <- res %>% as.data.frame() %>% filter(cl$cluster==1)
res2 <- res %>% as.data.frame() %>% filter(cl$cluster==2) 
ggplot(res, aes(x = ires, y= ..density.., color = cluster, group = cluster))+
    geom_histogram()+
    geom_density(adjust=1.5) +
    facet_wrap(~cluster) +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      axis.ticks.x=element_blank()
    )+
  theme_classic()

ggplot(res, aes(x = ires, color = cluster, group = cluster))+
    geom_boxplot() +
  coord_flip()+
  theme_classic()
```

At this time, the distribution within each subgoup is more homogeneous.

```{r}
t.test(res1$ires,res2$ires)
```

T test and boxplot shows that the subgroup makes sense. External data also verifies our results, where 59 patients experienced a relapse \citep{Lee2015IdentifyingAA}.
